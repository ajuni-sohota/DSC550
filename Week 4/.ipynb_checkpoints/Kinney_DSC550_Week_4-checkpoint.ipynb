{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Kinney DSC 550 Week 4: 4.2 Exercise: \n",
    "\n",
    "### Calculate Document Similarity \n",
    "*****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Week 4 PPT and the Sample Code for Jaccard Distance, TdifVectorizer and CountVectorizer. These are all excellent tools for text analysis.\n",
    "\n",
    "* Create a scenario of when and why you might want to determine if comments are positive or negative (or male/female or pass/fail or any other “binary” categorization). Also tell me how the results could be used.\n",
    "* You must read the data in from a file.\n",
    "* You must use some kind of vectorization method/tool (my example uses sklearn count.vectorizer but you can use any vectorization tool or Jaccard Distance.\n",
    "* Create some kind of a dictionary of sample words you will use to search /categorize your data.\n",
    "* Display the results.\n",
    "* For 10% extra credit…add something more to your program that relates to Ch 5-7!\n",
    "* Submit your code and a screenshot of the results.\n",
    "****************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a scenario of when and why you might want to determine if comments are positive or negative (or male/female or pass/fail or any other “binary” categorization). Also tell me how the results could be used.**\n",
    "\n",
    "There a number of scenarios I can think of that would be relevant for positive/negative comments. Advertising agencies immediately come to mind. We've all witnessed the scenarios where commercials appear that trigger a strong negative backlash due to taking political stands, or just outright offensiveness. Typically when these hit the small screen Twitter \"blows\" up. It would do ad agencies well to perform these sorts of analyses using Twitter's realtime streaming API. Ads with a strong negative response should be pulled, unless the company for which that ad was created cares more about political posturing or \"ad as art\" than they do about retaining brand loyalty (and, of course, profitability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>Apex should be ashamed of themselves. The new commercial is offensive and really puts the company in a bad light.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>What a fantastic commercial!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>It was really special to finally see a company show what it stand for.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>Take a breath people, the company can express its point of view. Personally I am ambivalent about it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>I have mixed emotions about the new ad; so far neither good or bad, have to give it more thought.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Day of Week  \\\n",
       "4  Friday       \n",
       "6  Sunday       \n",
       "2  Wednesday    \n",
       "5  Saturday     \n",
       "3  Thursday     \n",
       "\n",
       "                                                                                                            comments  \n",
       "4  Apex should be ashamed of themselves. The new commercial is offensive and really puts the company in a bad light.  \n",
       "6  What a fantastic commercial!                                                                                       \n",
       "2  It was really special to finally see a company show what it stand for.                                             \n",
       "5  Take a breath people, the company can express its point of view. Personally I am ambivalent about it.              \n",
       "3  I have mixed emotions about the new ad; so far neither good or bad, have to give it more thought.                  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the comments data\n",
    "df = pd.read_csv('data/DailyComments.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You must use some kind of vectorization method/tool, such as the sklearn count.vectorizer or Jaccard Distance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Words\n",
      "\n",
      "['about', 'ad', 'am', 'ambivalent', 'and', 'apex', 'ashamed', 'bad', 'be', 'breath', 'can', 'commercial', 'company', 'did', 'dynamite', 'emotions', 'express', 'fantastic', 'far', 'finally', 'for', 'give', 'glad', 'good', 'have', 'in', 'is', 'it', 'its', 'light', 'mixed', 'more', 'neither', 'new', 'of', 'offensive', 'or', 'people', 'personally', 'point', 'puts', 'really', 'see', 'should', 'show', 'so', 'special', 'stand', 'take', 'taking', 'the', 'themselves', 'thought', 'to', 'view', 'was', 'what', 'you']\n",
      "\n",
      "Identify Feature Words - Matrix View\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 2 0 0 1 0 0 1 1 1 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
      "  0 0 0 0 1 1 0 1 0 0 0 0 0 0 2 1 0 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0\n",
      "  0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      "                                                                                                                text  \\\n",
      "0  Did you see the new commercial it is so good!                                                                       \n",
      "1  I am glad to see Apex Dynamite taking a stand.                                                                      \n",
      "2  It was really special to finally see a company show what it stand for.                                              \n",
      "3  I have mixed emotions about the new ad; so far neither good or bad, have to give it more thought.                   \n",
      "4  Apex should be ashamed of themselves. The new commercial is offensive and really puts the company in a bad light.   \n",
      "5  Take a breath people, the company can express its point of view. Personally I am ambivalent about it.               \n",
      "6  What a fantastic commercial!                                                                                        \n",
      "\n",
      "   positive1  positive2  positive3  negative1  negative2  TotScore  \n",
      "0  1          0          0          0          0          1         \n",
      "1  0          0          0          0          0          0         \n",
      "2  0          1          0          0          0          1         \n",
      "3  1          0          0          1          0          0         \n",
      "4  0          0          0          1          1         -2         \n",
      "5  0          0          0          0          0          0         \n",
      "6  0          0          1          0          0          1         \n",
      "\n",
      "Overall Score:   1\n"
     ]
    }
   ],
   "source": [
    "corpus = df['comments']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Vectorized Words\")\n",
    "print(\"\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"\")\n",
    "print(\"Identify Feature Words - Matrix View\")\n",
    "print(\"\")\n",
    "print( X.toarray())\n",
    "\n",
    "df = pd.DataFrame({'text' : corpus})\n",
    "\n",
    "#check for positive words and negative words\n",
    "df['positive1'] = df.text.str.count('good')\n",
    "df['positive2']= df.text.str.count('special')\n",
    "df['positive3']= df.text.str.count('fantastic')\n",
    "df['negative1'] = df.text.str.count('bad')\n",
    "df['negative2'] = df.text.str.count('ashamed')\n",
    "df['TotScore'] = (df.positive1 + df.positive2 + df.positive3) - (df.negative1 + df.negative2)\n",
    "\n",
    "print(\"\")\n",
    "print(df)\n",
    "\n",
    "Z = sum(df['TotScore'])\n",
    "print(\"\")\n",
    "print(\"Overall Score:  \",Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 10% extra credit…add something more to your program that relates to Ch 5-7!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5: Remove \"stop\" words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "tokenized_words = vectorizer.get_feature_names()\n",
    "words_cleaned = [word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'ambivalent',\n",
       " 'apex',\n",
       " 'ashamed',\n",
       " 'bad',\n",
       " 'breath',\n",
       " 'commercial',\n",
       " 'company',\n",
       " 'dynamite',\n",
       " 'emotions',\n",
       " 'express',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'finally',\n",
       " 'give',\n",
       " 'glad',\n",
       " 'good',\n",
       " 'light',\n",
       " 'mixed',\n",
       " 'neither',\n",
       " 'new',\n",
       " 'offensive',\n",
       " 'people',\n",
       " 'personally',\n",
       " 'point',\n",
       " 'puts',\n",
       " 'really',\n",
       " 'see',\n",
       " 'show',\n",
       " 'special',\n",
       " 'stand',\n",
       " 'take',\n",
       " 'taking',\n",
       " 'thought',\n",
       " 'view']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7 Tagging Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('about', 'IN'),\n",
       " ('ad', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('ambivalent', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('apex', 'NN'),\n",
       " ('ashamed', 'VBD'),\n",
       " ('bad', 'JJ'),\n",
       " ('be', 'VB'),\n",
       " ('breath', 'VBN'),\n",
       " ('can', 'MD'),\n",
       " ('commercial', 'JJ'),\n",
       " ('company', 'NN'),\n",
       " ('did', 'VBD'),\n",
       " ('dynamite', 'JJ'),\n",
       " ('emotions', 'NNS'),\n",
       " ('express', 'RBR'),\n",
       " ('fantastic', 'JJ'),\n",
       " ('far', 'RB'),\n",
       " ('finally', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('give', 'JJ'),\n",
       " ('glad', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('have', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('its', 'PRP$'),\n",
       " ('light', 'JJ'),\n",
       " ('mixed', 'VBN'),\n",
       " ('more', 'RBR'),\n",
       " ('neither', 'JJ'),\n",
       " ('new', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('offensive', 'JJ'),\n",
       " ('or', 'CC'),\n",
       " ('people', 'NNS'),\n",
       " ('personally', 'RB'),\n",
       " ('point', 'VBP'),\n",
       " ('puts', 'NNS'),\n",
       " ('really', 'RB'),\n",
       " ('see', 'VB'),\n",
       " ('should', 'MD'),\n",
       " ('show', 'VB'),\n",
       " ('so', 'RB'),\n",
       " ('special', 'JJ'),\n",
       " ('stand', 'NN'),\n",
       " ('take', 'VB'),\n",
       " ('taking', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('themselves', 'PRP'),\n",
       " ('thought', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('view', 'VB'),\n",
       " ('was', 'VBD'),\n",
       " ('what', 'WP'),\n",
       " ('you', 'PRP')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Use pre-trained part of speech tagger\n",
    "string_data = ' '.join(tokenized_words)\n",
    "text_tagged = pos_tag(word_tokenize(string_data))\n",
    "\n",
    "# Show parts of speech\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "**References**  \n",
    "\n",
    "Albon, Chris. Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning . O'Reilly Media. Kindle Edition. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
